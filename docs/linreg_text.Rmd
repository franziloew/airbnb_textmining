---
title: "Using Text Mining To Predict Prices - Part 2"
subtitle: "Linear Regression with Text Data"
output:  
  html_document:
    theme: "lumen"
    highlight: "tango"
    code_folding: show
    self_contained: true
---
```{r message=FALSE, warning=FALSE}
library(readr)
library(dplyr)
library(tidytext)
library(tm)
library(ggplot2)
library(ggridges)
require(quanteda)

rm(list=ls())
col <- RColorBrewer::brewer.pal(5, "Dark2")

options(stringsAsFactors = FALSE)
knitr::opts_chunk$set(message=FALSE, warning=FALSE)
```


```{r}
load(file="../output/prep2.Rda")
```

Can we improve the model from [part 2](https://franziloew.github.io/airbnb_textmining/linreg_text.html) including the description text of the listing on the right-hand side of the model?

As before, we wish to predict the price $y_i$ for each Airbnb listing $i$ based on some independent variables $X$. In this model the text counts $c_i$ of each listing description are used. This is a regression problem like any other, except that the high-dimensionality of $c_i$ makes OLS and other standard techniques infeasible. 

Here, we have $d=20.637$ documents (aka Airbnb listings) each of which is $w$ words long. Each word is drawn from a vocabulary of $p=33.469$ possible words. The unique representation of each document has dimension $p^w$. A common strategy to deal with the high-dimensionality of text data is the estimation of penalized linear models, particularly with $L_1$ penalization (Gentzkow, 2017).

However, a first step to use textdata in a prediction model is to convert it to a Document Term Matrix, where each row is a observation (document) and each column is a unique term. 

```{r}
corp <- Corpus(VectorSource(df$text_cleaned))
dtm <- DocumentTermMatrix(corp)

dtm
```

The first five observations of the Document Term Matrix look like this:

```{r}
inspect(dtm[1:5, 1:10])
```

Matrices in text analysis problems tend to be very sparse. That is, most of the elements are zero, which implies that they have many parameters that are uninformative. (In contrast: a dense matrix is a matrix in which most of the elements are nonzero.)

Sparsity can be reduced by removing terms that occur very frequently. This tends to have the effect of both reducing overfitting and improving the predictive abilities of the model.

```{r}
# Here we are reducing the sparsity of the document-term matrix so that the sparsity (% of non-zeros) is a maximum of 99%.
dtm<-removeSparseTerms(dtm,0.99)
dtm
```

```{r}
inspect(dtm[1:5, 1:10])
```

```{r}
# Convert to Dataframe
dtm.df <- as.matrix(dtm) %>%
  as.data.frame()

# Merge with orignal dataframe
dtm.df$document <-as.integer(rownames(dtm.df))
df$document <- as.integer(rownames(df))

df.reg <- dtm.df %>%
  left_join(df %>%
              select(document, price),
            by = "document") %>%
  filter(price != 0) %>%
  mutate(log_price = log(price)) %>%
  select(-document, -price)
```

## (1) Training / Test

```{r}
#create test and training sets
bound <- floor((nrow(df.reg)/4)*3)         #define % of training and test set

df.reg <- df.reg[sample(nrow(df.reg)), ]           #sample rows 
df.train <- df.reg[1:bound, ]              #get training set
df.test <- df.reg[(bound+1):nrow(df.reg), ]    #get test set
```

In [part 1](https://franziloew.github.io/airbnb_textmining/data_prep.html) we saw that our dependent variable is left-skewed. This is somehow problematic, as the linear regression assumes normal distributed data. A common strategy to deal with left-skewed data is to take the logarithmic values (log-level model). Lets have a look at the log distributions.

```{r fig.height=3, fig.width=8}
p1 <- ggplot(df.train, aes(log_price)) +
  geom_density(fill = col[3], color = "white") +
  labs(title = "Train Data")

p2 <- ggplot(df.test, aes(log_price)) +
  geom_density(fill = col[3], color = "white") +
  labs(y="", title = "Test Data")

gridExtra::grid.arrange(p1, p2, nrow = 1)
```

## Linear Regression

### (2) Estimate Training Data

```{r}
lm.train <- lm(log_price~., data = df.train)

summary(lm.train)
```


**F stats:** I can reject the null hypothesis that all of the regression coefficients are equal to zero (p-value < 0.01).

**Adjusted R-squared:** Nearly $32\%$ of the variance of the price can be explained by our model. 

**Coefficients**

The inevitable multicollinearity makes individual parameters difficult to interpret. However, it is still a good exercise to look at the most important coefficients to see if they make intuitive sense in the context of a particular application. "Most important" can be defined in a number of ways. Here, I will rank the estimated coefficients by their absolute value.

The plot below show all terms with a p-value < 0.01.

```{r fig.height=12, fig.width=6}
library(broom)

tidy(lm.train) %>%
  filter(p.value < 0.01) %>%
  filter(term != "(Intercept)") %>%
  mutate(pos = factor(ifelse(estimate>=0,1,0))) %>%
  #top_n(20,estimate) %>%
  ggplot(aes(reorder(term, estimate),estimate,
             fill = pos)) +
  geom_col(show.legend = F, alpha = 0.8) +
  coord_flip() +
  scale_fill_manual(values = c(col[1],col[2])) +
  labs(x="Estimate", y="", title ="Coefficients with p<0.01")
```


```{r fig.height=8, fig.width=10}
par(mfrow=c(2,2))
plot(lm.train)
```

### (3) Make Predictions & Evaluate the model

Next, I use the trained model to make predictions on my test data. 

```{r}
pred <- predict(lm.train, newdata = df.test)
```

```{r}
# Combine predictions with test dataframe
pred <- as.data.frame(pred) 
pred$listing <- as.numeric(rownames(pred))
df.test$listing <- as.numeric(rownames(df.test))

pred.df <- left_join(pred, df.test %>%
                       select(log_price, listing),
                     by = "listing") %>%
  mutate(error = log_price-pred)
```

A plot of the predicted values for the price agains the actual values shows the explanatory power of the prediction model.  

```{r}
ggplot(pred.df, aes(pred, log_price)) +
  geom_point(alpha = 0.8) +
  geom_smooth(method = lm) +
  labs(x="Predcted y", y="Actual y")
```

**RMSE**

```{r}
rmse <- function(error) {
  sqrt(mean(error^2))
  }

print(paste0("The RMSE is: ", rmse(pred.df$error)))
```

The variance of the price in the test dataset is helpful to get a better understanding of the RMSE value:

```{r}
print(paste0("The variance of log(price) is: ",var(df.test$log_price)))
```


## Penalized Regression
```{r}
library(glmnet)
```

Ridge regression and the lasso are closely related, but only the Lasso has the ability to select predictors. Like OLS, ridge attempts to minimize residual sum of squares of predictors in a given model. However, ridge regression includes an additional ‘shrinkage’ term – the square of the coefficient estimate – which shrinks the estimate of the coefficients towards zero. The impact of this term is controlled by another term, lambda (determined seperately). Two interesting implications of this design are the facts that when $\lambda = 0$ the OLS coefficients are returned and when $\lambda = ∞$, coefficients will approach zero.

<!-- # ```{r} -->
<!-- # lambda <- 10^seq(10, -2, length = 100) -->
<!-- # ``` -->
<!-- #  -->
<!-- # ```{r} -->
<!-- # #ridge -->
<!-- # ridge.mod <- glmnet(x, y, alpha = 0, lambda = lambda) -->
<!-- # predict(ridge.mod, s = 0, type = 'coefficients')[1:10,] -->
<!-- # ``` -->
<!-- #  -->
<!-- # The differences here are nominal. Let's see if we can use ridge to improve on the OLS estimate. -->
<!-- #  -->
<!-- # ```{r} -->
<!-- # textlm <- lm(price~., data = dtm.df, subset = train) -->
<!-- #  -->
<!-- # ridge.mod <- glmnet(x[train,], y[train], alpha = 0, lambda = lambda) -->
<!-- #  -->
<!-- # #find the best lambda from our list via cross-validation -->
<!-- # cv.out <- cv.glmnet(x[train,], y[train], alpha = 0) -->
<!-- # ``` -->
<!-- #  -->
<!-- # ```{r} -->
<!-- # bestlam <- cv.out$lambda.min -->
<!-- # ``` -->
<!-- #  -->
<!-- # ```{r} -->
<!-- # #make predictions -->
<!-- # ridge.pred <- predict(ridge.mod, s = bestlam, newx = x[test,]) -->
<!-- # s.pred <- predict(swisslm, newdata = swiss[test,]) -->
<!-- # #check MSE -->
<!-- # mean((s.pred-ytest)^2) -->
<!-- # ``` -->
<!-- #  -->
<!-- # ```{r} -->
<!-- # mean((ridge.pred-ytest)^2) -->
<!-- # ``` -->
<!-- #  -->
<!-- # Ridge performs better for this data according to the MSE. -->
<!-- #  -->
<!-- # ```{r} -->
<!-- # #a look at the coefficients -->
<!-- # out = glmnet(x[train,],y[train],alpha = 0) -->
<!-- # predict(ridge.mod, type = "coefficients", s = bestlam)[1:6,] -->
<!-- # ``` -->
<!-- #  -->
<!-- # As expected, most of the coefficient estimates are more conservative. -->
<!-- #  -->
<!-- # Let's have a look at the lasso. The big difference here is in the shrinkage term – the lasso takes the absolute value of the coefficient estimates. -->
<!-- #  -->
<!-- # ```{r} -->
<!-- # lasso.mod <- glmnet(x[train,], y[train], alpha = 1,  -->
<!-- #                     lambda = lambda) -->
<!-- #  -->
<!-- # plot(lasso.mod, xvar = "lambda") -->
<!-- # ``` -->
<!-- #  -->
<!-- # To interpet this plot, recall the optimization problem the lasso solves -->
<!-- #  -->
<!-- # $$ -->
<!-- # \beta_{LASSO}=\min_{\beta}(y-x\beta)'(y-x\beta)+\lambda \sum_j|\beta_j| -->
<!-- # $$ -->
<!-- #  -->
<!-- # so $\lambda$ is the penalty or the Lagrange multiplier. Setting $\lambda = 0$ yields the familiar minimization of squared residuals while for greater values, some of the coefficients will be set to zero. As $\lambda→∞$, all the coefficents will be set to zero.  -->
<!-- #  -->
<!-- # Next, we can choose the best lambda using crossvalidation (splitting the sample into training and validation sets and choosing the value of lambda with which the error of prediction is minimal.) -->
<!-- #  -->
<!-- # ```{r} -->
<!-- # crossval <-  cv.glmnet(x = x, y = y) -->
<!-- # plot(crossval) -->
<!-- # ``` -->
<!-- #  -->
<!-- # ```{r} -->
<!-- # penalty <- crossval$lambda.min #optimal lambda -->
<!-- # penalty #minimal shrinkage -->
<!-- # ``` -->
<!-- #  -->
<!-- # ```{r} -->
<!-- # fit1 <-glmnet(x = x, y = y, alpha = 1, lambda = penalty ) -->
<!-- # plot(fit1) -->
<!-- # ``` -->
