---
title: "Using Text Mining To Predict Prices - Part 2"
subtitle: "Linear Regression"
output:  
  html_document:
    theme: "lumen"
    highlight: "tango"
    code_folding: show
    self_contained: true
---
```{r message=FALSE, warning=FALSE, include=FALSE}
library(readr)
library(dplyr)
library(tidytext)
library(tm)
library(ggplot2)
library(ggridges)

rm(list=ls())
col <- RColorBrewer::brewer.pal(5, "Dark2")

options(stringsAsFactors = FALSE)
knitr::opts_chunk$set(message=FALSE, warning=FALSE)

load(file = "../output/prep2.Rda")
```

Based on the literature dealing with Airbnb listing prices (e.g. [Gibbs et al. (2018)](https://www.tandfonline.com/doi/abs/10.1080/10548408.2017.1308292?journalCode=wttm20), [Teubner et al. (2017)](https://www.ceeol.com/search/article-detail?id=596092)), I assume that the following variables have an effect on the price:

- city: The location of the listing

- reviews: Number of reviews of a listing

- accommodates: Number of guests the listing is able to host

- overall_satisfaction: The overall rating of a listing

- pic_count: The number of fotos of the listing

- language: langugage of the description text of the listing

- room_type: (i) Entire home, (ii) Private room, (iii) Shared room

```{r}
#reduce dataframe

df.reg <- df %>%
  select(city, price, overall_satisfaction, pic_count, 
         language,
         room_type, bed_type, reviews, accommodates) %>%
  mutate(city = as.factor(city),
         language = as.factor(language),
         room_type = as.factor(room_type),
         log_price = log(price)) %>%
  filter(price != 0)
```

Check correlations to see if we have a problem of multicollinearity:

```{r}
df.reg %>%
  select(price, overall_satisfaction, pic_count,
         reviews, accommodates) %>%
  cor() %>%
  knitr::kable()
```

To make predictions, the following steps are applied:  

1. Split the sample dataset into training and testing dataset.

2. Estimate a linear model using the training data (make a prediction based on the model).

3. Use test dataset to evaluate the model: predict the ‘test’ observation and compare between predicted response and actual response value (RMSE explains on an average how much of the predicted value will be from the actual value) 

## (1) Training / Test

```{r}
#create test and training sets
bound <- floor((nrow(df.reg)/4)*3)         #define % of training and test set

df.reg <- df.reg[sample(nrow(df.reg)), ]           #sample rows 
df.train <- df.reg[1:bound, ]              #get training set
df.test <- df.reg[(bound+1):nrow(df.reg), ]    #get test set
```

In [part 1](https://franziloew.github.io/airbnb_textmining/data_prep.html) we saw that our dependent variable is left-skewed. This is somehow problematic, as the linear regression assumes normal distributed data. A common strategy to deal with left-skewed data is to take the logarithmic values (log-level model). Lets have a look at the log distributions.

```{r fig.height=3, fig.width=8}
p1 <- ggplot(df.train, aes(log(price))) +
  geom_density(fill = col[3], color = "white") +
  labs(title = "Train Data")

p2 <- ggplot(df.test, aes(log(price))) +
  geom_density(fill = col[3], color = "white") +
  labs(y="", title = "Test Data")

gridExtra::grid.arrange(p1, p2, nrow = 1)
```

## (2) Estimate Training Data 

```{r}
lm.train <- lm(log_price ~ overall_satisfaction + reviews
              + room_type + accommodates + language
              + city + pic_count
             , data = df.train,
             na.action=na.pass)

summary(lm.train) 
```

**F stats** 

This defines the collective effect of all predictor variables on the response variable. The null hypothesis states that the model with no independent variables fits the data as well as your model. This can be rejected as the p-value is very small.

**Multiple R-squared**

The $R^2$ (multiple-R-squared) value indicates what proportion of the variation of the endogenous variable (the price) is explained by the model. The adjusted R-squared is normalized by the number of exogenous variables (n-1-k). In this case, the value is 0.4841 meaning that nearly $49\%$ of the variance of the price can be explained by our model. 

**Coefficients**

As I am estimating a log-level regression model ($ln(y)=X\beta+\epsilon$), the interpretation of the coefficients is as follows: $\% \Delta y=100\beta \Delta X$*. E.g. the coefficient of the rating variabels (overall_satisfaction) can be interpreted as "if the rating rises by 1 star, we’d expect the price to change by $0.8 \%$, assuming that all other variables remain the same”. Interestingly, the number of reviews has a negative effect on the price. Although this result may not seem intuitive at first glance, it confirms the results of earlier studies on Airbnb listing prices [Teubner et al. (2017)](https://www.ceeol.com/search/article-detail?id=596092)). Overall, all exogenous variables are highly significant, except for the dummy variables for Cologne and Stuttgart. The variable with the strongest effect is the dummy variable for shared room. If the listing offers a shared room, the price decreases by $68\%$ compared to the case were the listing would offer the entire home and all other variables remain the same. 

(*Technically, the interpretation is $\% \Delta y=100(e^{\beta}-1)$ but the previously mentioned interpretation is approximately true for values $-0.1 < \beta < 0.1$) 

### Visualising Residuals

```{r fig.height=8, fig.width=10}
par(mfrow=c(2,2))
plot(lm.train)
```

**Fitted vs Residual graph**

In this plot each point is one listing, where the prediction made by the model is on the x-axis, and the accuracy of the prediction is on the y-axis. The distance from the line at 0 is how bad the prediction was for that value.

Since...

    Residual = Observed – Predicted

...positive values for the residual (on the y-axis) mean the prediction was too low, and negative values mean the prediction was too high; 0 means the guess was exactly correct.

Ideally your plot of the residuals to meet the following requirements:

  (1) they’re pretty symmetrically distributed, tending to cluster towards the middle of the plot.
  
  (2) they’re clustered around the lower single digits of the y-axis.
  
  (3) in general there aren’t clear patterns.

**Normal Q-Q Plot**

Q-Q plot shows whether the residuals are normally distributed. Ideally, the plot should be on the dotted line. If the Q-Q plot is not on the line then models need to be reworked to make the residual normal. In the above plot, we see that most of the plots are on the line except the extreme points (beginning and end).

**Scale-Location**

This shows how the residuals are spread and whether the residuals have an equal variance or not.

**Residuals vs Leverage**

The plot helps to find influential observations. Here we need to check for points that are outside the dashed line (Cooks distance). A point outside the dashed line will be influential point and removal of that will affect the regression coefficients.

## (3) Make Predictions & Evaluate the model

Next, I use the trained model to make predictions on my test data. 

```{r}
pred <- predict(lm.train, newdata = df.test)
```

```{r}
# Combine predictions with test dataframe
pred <- as.data.frame(pred) 
pred$listing <- as.numeric(rownames(pred))
df.test$listing <- as.numeric(rownames(df.test))

pred.df <- left_join(pred, df.test %>%
                       select(log_price, listing),
                     by = "listing") %>%
  mutate(error = log_price-pred)
```

A plot of the predicted values for the price agains the actual values shows the explanatory power of the prediction model.  

```{r}
ggplot(pred.df, aes(pred, log_price)) +
  geom_point(alpha = 0.8) +
  geom_smooth(method = lm) +
  labs(x="Predcted y", y="Actual y")
```

The RMSE (root mean squared error), also called RMSD (root mean squared deviation), is used to evaluate models by summarizing the differences between the actual (observed) and predicted values. As the square root of a variance, RMSE can be interpreted as the standard deviation of the unexplained variance, and has the useful property of being in the same units as the response variable. Lower values of RMSE indicate better fit. 

$$
\text{RMSE} =\sqrt{\frac{1}{n}\sum^n_{i=1}(y_i-\hat{y_i})^2}
$$

```{r}
rmse <- function(error) {
  sqrt(mean(error^2))
  }

print(paste0("The RMSE is: ", rmse(pred.df$error)))
```

The variance of the price in the test dataset is helpful to get a better understanding of the RMSE value:

```{r}
print(paste0("The variance of log(price) is: ",var(df.test$log_price)))
```

In [part 3](https://franziloew.github.io/airbnb_textmining/linreg_text.html) I will check if the description test of the listings give a better prediction power than the structural features estimated here.  
